{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc5d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khointn/anaconda3/envs/apple/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "\n",
    "config = {\n",
    "    \"model\": \"Qwen/Qwen3-0.6B\",   # or \"meta-llama/Meta-Llama-3-8B\"\n",
    "    \"output_dir\": \"./qwen3-0.6b-lora\",\n",
    "    \"max_input_tokens\": 384,\n",
    "    \"max_target_tokens\": 128,\n",
    "    \"batch_size\": 2,\n",
    "    \"grad_accum\": 8,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epochs\": 4,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"eval_steps\": 400,\n",
    "    \"logging_steps\": 400,\n",
    "    \"seed\": 42,\n",
    "    \"load_4bit\": True,\n",
    "    \"text_col\": \"text\",\n",
    "    \"summary_col\": \"summary\",\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"optim\": \"adamw_bnb_8bit\"\n",
    "}\n",
    "\n",
    "# Prompt template\n",
    "INSTR_PREFIX = (\n",
    "    \"You are a helpful assistant that writes concise, faithful summaries of legislative text.\\n\\n\"\n",
    "    \"Task: Read the following bill text and write a clear, accurate summary.\\n\\n\"\n",
    "    \"Bill Text:\\n\"\n",
    ")\n",
    "RESPONSE_PREFIX = \"\\n\\nSummary:\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Build samples & masking\n",
    "# ---------------------------\n",
    "def format_example(ex: Dict[str, str], text_col: str, sum_col: str) -> Tuple[str, str]:\n",
    "    src = (ex.get(text_col) or \"\").strip()\n",
    "    tgt = (ex.get(sum_col) or \"\").strip()\n",
    "    prompt = f\"{INSTR_PREFIX}{src}{RESPONSE_PREFIX} \"\n",
    "    return prompt, tgt\n",
    "\n",
    "def tokenize_and_mask(example: Dict[str, str], tokenizer, cfg) -> Dict[str, List[int]]:\n",
    "    prompt, target = format_example(example, cfg[\"text_col\"], cfg[\"summary_col\"])\n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"][:cfg[\"max_input_tokens\"]]\n",
    "    target_ids = tokenizer(target, add_special_tokens=False)[\"input_ids\"][:cfg[\"max_target_tokens\"]]\n",
    "\n",
    "    input_ids = prompt_ids + target_ids + [tokenizer.eos_token_id]\n",
    "    input_ids = input_ids[:cfg[\"max_input_tokens\"] + cfg[\"max_target_tokens\"]]\n",
    "\n",
    "    labels = [-100] * len(prompt_ids) + target_ids + [tokenizer.eos_token_id]\n",
    "    labels = labels[:cfg[\"max_input_tokens\"] + cfg[\"max_target_tokens\"]]\n",
    "\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Collator (pads labels with -100)\n",
    "# ---------------------------\n",
    "@dataclass\n",
    "class DataCollatorForCausalLMWithMaskedLabels:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    label_pad_token_id: int = -100\n",
    "    pad_to_multiple_of: int = 8\n",
    "\n",
    "    def __call__(self, features):\n",
    "        max_len = max(len(f[\"input_ids\"]) for f in features)\n",
    "        if self.pad_to_multiple_of and max_len % self.pad_to_multiple_of:\n",
    "            max_len = ((max_len // self.pad_to_multiple_of) + 1) * self.pad_to_multiple_of\n",
    "\n",
    "        input_ids, attn, labels = [], [], []\n",
    "        for f in features:\n",
    "            pad = max_len - len(f[\"input_ids\"])\n",
    "            input_ids.append(f[\"input_ids\"] + [self.tokenizer.pad_token_id] * pad)\n",
    "            attn.append(f[\"attention_mask\"] + [0] * pad)\n",
    "            labels.append(f[\"labels\"] + [self.label_pad_token_id] * pad)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics (ROUGE + BLEU)\n",
    "# ---------------------------\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleu_metric  = evaluate.load(\"bleu\")\n",
    "\n",
    "def _postprocess_text(preds: List[str], labels: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    preds  = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    preds  = [\"\\n\".join(p.splitlines()) for p in preds]\n",
    "    labels = [\"\\n\".join(l.splitlines()) for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds  = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds, decoded_labels = _postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    rouge = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    bleu = bleu_metric.compute(predictions=decoded_preds, references=[[r] for r in decoded_labels])\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": round(rouge[\"rouge1\"] * 100, 4),\n",
    "        \"rouge2\": round(rouge[\"rouge2\"] * 100, 4),\n",
    "        \"rougeL\": round(rouge[\"rougeL\"] * 100, 4),\n",
    "        \"rougeLsum\": round(rouge[\"rougeLsum\"] * 100, 4),\n",
    "        \"bleu\": round(bleu[\"bleu\"] * 100, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca3f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(config[\"seed\"])\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model\"], use_fast=True, trust_remote_code=True, truncate=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c599002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = load_dataset(\"billsum\")\n",
    "# val_size = 2000 if len(raw[\"train\"]) > 40000 else max(1000, int(0.05 * len(raw[\"train\"])))\n",
    "val_size = 2000\n",
    "split = raw[\"train\"].train_test_split(test_size=val_size, seed=config[\"seed\"])\n",
    "train_ds_raw, val_ds_raw = split[\"train\"], split[\"test\"]\n",
    "\n",
    "max_len = config[\"max_input_tokens\"] + config[\"max_target_tokens\"]\n",
    "train_ds = train_ds_raw.map(lambda ex: tokenize_and_mask(ex, tokenizer, config),\n",
    "                            remove_columns=train_ds_raw.column_names, desc=\"Tokenize train\")\n",
    "val_ds   = val_ds_raw.map(lambda ex: tokenize_and_mask(ex, tokenizer, config),\n",
    "                          remove_columns=val_ds_raw.column_names, desc=\"Tokenize eval\")\n",
    "\n",
    "data_collator = DataCollatorForCausalLMWithMaskedLabels(tokenizer=tokenizer, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a217d",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93de3861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_kwargs = {}\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else None\n",
    "if config[\"load_4bit\"]:\n",
    "    bnb_kwargs = dict(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config[\"model\"],\n",
    "    torch_dtype=dtype if not config[\"load_4bit\"] else None,\n",
    "    trust_remote_code=True,\n",
    "    **bnb_kwargs,\n",
    ")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=config[\"lora_r\"],\n",
    "    lora_alpha=config[\"lora_alpha\"],\n",
    "    lora_dropout=config[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\",\"W_pack\"],\n",
    ")\n",
    "model = get_peft_model(base_model, lora_cfg)\n",
    "\n",
    "model.generation_config.update(\n",
    "    max_new_tokens=config[\"max_target_tokens\"],\n",
    "    num_beams=1,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65132877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Training args\n",
    "# ---------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    num_train_epochs=config[\"epochs\"],\n",
    "    per_device_train_batch_size=config[\"batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"batch_size\"],\n",
    "    gradient_accumulation_steps=config[\"grad_accum\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=config[\"warmup_ratio\"],\n",
    "    logging_steps=config[\"logging_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=config[\"eval_steps\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    # eval_steps=config[\"eval_steps\"],\n",
    "\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    seed=config[\"seed\"],\n",
    "    optim=config[\"optim\"],\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Trainer\n",
    "# ---------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save LoRA adapter only\n",
    "adapter_dir = os.path.join(config[\"output_dir\"], \"lora_adapter\")\n",
    "os.makedirs(adapter_dir, exist_ok=True)\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(config[\"output_dir\"])\n",
    "\n",
    "print(f\"\\nDone. LoRA adapter saved to: {adapter_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467093a",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c74cf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.69it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# paths\n",
    "base_model_name = \"Qwen/Qwen3-1.7B\"   # or whatever base model you used\n",
    "ckpt_path = \"/home/khointn/summarization_adapter/nbs/qwen3-1.7b-lora-384ctx/checkpoint-600\"\n",
    "\n",
    "# reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True, trust_remote_code=True, truncate=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# reload base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# attach LoRA weights from checkpoint\n",
    "model = PeftModel.from_pretrained(base_model, ckpt_path)\n",
    "\n",
    "# (optional) merge for faster inference\n",
    "try:\n",
    "    model = model.merge_and_unload()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# put on device\n",
    "device = \"cuda\" # if torch.cuda.is_available() else \"cpu\"\n",
    "model.generation_config.update(\n",
    "    max_new_tokens=config[\"max_target_tokens\"],\n",
    "    num_beams=1,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7d9704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(35.4827), 'rouge2': np.float64(35.2095), 'rougeL': np.float64(35.469), 'rougeLsum': np.float64(35.3707), 'bleu': 20.3816}\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Reuse your tokenizer, model, and eval_ds (tokenized with input_ids, attention_mask, labels)\n",
    "# Assumptions:\n",
    "#   - eval_ds[i][\"input_ids\"] / [\"attention_mask\"] / [\"labels\"] are python lists of ints\n",
    "#   - tokenizer has pad_token_id and eos_token_id set\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleu_metric  = evaluate.load(\"bleu\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def streaming_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_ds,\n",
    "    *,\n",
    "    n_samples=256,              # limit for quick eval\n",
    "    batch_size=1,               # keep tiny\n",
    "    max_input_ctx=None,         # hard cap input ctx tokens (optional)\n",
    "    max_new_tokens=128,         # cap generated length\n",
    "    device=None,\n",
    "    clear_cuda_every=8,         # free CUDA cache periodically\n",
    "):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # Process in small slices, no big lists\n",
    "    end = min(n_samples, len(eval_ds))\n",
    "    for start in range(0, end, batch_size):\n",
    "        stop = min(start + batch_size, end)\n",
    "        batch = eval_ds.select(range(start, stop))\n",
    "\n",
    "        # 1) Truncate inputs (optional safety cap)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention = batch[\"attention_mask\"]\n",
    "        labels    = batch[\"labels\"]\n",
    "\n",
    "        if max_input_ctx is not None:\n",
    "            input_ids = [ids[:max_input_ctx] for ids in input_ids]\n",
    "            attention = [att[:max_input_ctx] for att in attention]\n",
    "            # labels are not fed to generate, no need to truncate here\n",
    "\n",
    "        # 2) Pad this batch\n",
    "        max_len = max(len(x) for x in input_ids)\n",
    "        pad_id = tokenizer.pad_token_id\n",
    "        batch_input = [x + [pad_id]*(max_len - len(x)) for x in input_ids]\n",
    "        batch_attn  = [x + [0]*(max_len - len(x))  for x in attention]\n",
    "\n",
    "        batch_input = torch.tensor(batch_input, dtype=torch.long, device=device)\n",
    "        batch_attn  = torch.tensor(batch_attn,  dtype=torch.long, device=device)\n",
    "\n",
    "        # 3) Generate\n",
    "        gen = model.generate(\n",
    "            input_ids=batch_input,\n",
    "            attention_mask=batch_attn,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        # 4) Decode predictions (streaming: only this batch)\n",
    "        decoded_preds = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "        # 5) Prepare and decode labels for this batch (replace -100 with pad for decode)\n",
    "        #    (We never feed labels to the model here, just for metrics.)\n",
    "        max_lab = max(len(l) for l in labels)\n",
    "        labels_padded = [\n",
    "            [tok if tok != -100 else pad_id for tok in l] + [pad_id]*(max_lab - len(l))\n",
    "            for l in labels\n",
    "        ]\n",
    "        decoded_refs = tokenizer.batch_decode(\n",
    "            torch.tensor(labels_padded, dtype=torch.long), skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # 6) Minimal post-processing for ROUGE-Lsum\n",
    "        decoded_preds = [p.strip() for p in decoded_preds]\n",
    "        decoded_refs  = [r.strip() for r in decoded_refs]\n",
    "        decoded_preds = [\"\\n\".join(p.splitlines()) for p in decoded_preds]\n",
    "        decoded_refs  = [\"\\n\".join(r.splitlines()) for r in decoded_refs]\n",
    "\n",
    "        # 7) Stream into metrics (no big lists kept)\n",
    "        rouge_metric.add_batch(predictions=decoded_preds, references=decoded_refs)\n",
    "        bleu_metric.add_batch(predictions=decoded_preds, references=[[r] for r in decoded_refs])\n",
    "\n",
    "        # 8) Free everything we can\n",
    "        del batch_input, batch_attn, gen\n",
    "        if torch.cuda.is_available() and (start // batch_size) % clear_cuda_every == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # 9) Compute final metrics\n",
    "    rouge = rouge_metric.compute(use_stemmer=True)\n",
    "    bleu  = bleu_metric.compute()\n",
    "    return {\n",
    "        \"rouge1\": round(rouge[\"rouge1\"] * 100, 4),\n",
    "        \"rouge2\": round(rouge[\"rouge2\"] * 100, 4),\n",
    "        \"rougeL\": round(rouge[\"rougeL\"] * 100, 4),\n",
    "        \"rougeLsum\": round(rouge[\"rougeLsum\"] * 100, 4),\n",
    "        \"bleu\": round(bleu[\"bleu\"] * 100, 4),\n",
    "    }\n",
    "\n",
    "# ---- Usage example (super light) ----\n",
    "scores = streaming_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    val_ds,\n",
    "    n_samples=100,           # evaluate on a tiny slice\n",
    "    batch_size=1,            # minimize peak RAM\n",
    "    max_input_ctx=512,       # hard cap context if your eval_ds has longer\n",
    "    max_new_tokens=128,      # short generations\n",
    ")\n",
    "print(scores)\n",
    "# qwen 1.7-384ctx checkpoint 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3411db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(34.2349), 'rouge2': np.float64(33.9942), 'rougeL': np.float64(34.2084), 'rougeLsum': np.float64(34.1441), 'bleu': 19.5966}\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Reuse your tokenizer, model, and eval_ds (tokenized with input_ids, attention_mask, labels)\n",
    "# Assumptions:\n",
    "#   - eval_ds[i][\"input_ids\"] / [\"attention_mask\"] / [\"labels\"] are python lists of ints\n",
    "#   - tokenizer has pad_token_id and eos_token_id set\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleu_metric  = evaluate.load(\"bleu\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def streaming_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_ds,\n",
    "    *,\n",
    "    n_samples=256,              # limit for quick eval\n",
    "    batch_size=1,               # keep tiny\n",
    "    max_input_ctx=None,         # hard cap input ctx tokens (optional)\n",
    "    max_new_tokens=128,         # cap generated length\n",
    "    device=None,\n",
    "    clear_cuda_every=8,         # free CUDA cache periodically\n",
    "):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # Process in small slices, no big lists\n",
    "    end = min(n_samples, len(eval_ds))\n",
    "    for start in range(0, end, batch_size):\n",
    "        stop = min(start + batch_size, end)\n",
    "        batch = eval_ds.select(range(start, stop))\n",
    "\n",
    "        # 1) Truncate inputs (optional safety cap)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention = batch[\"attention_mask\"]\n",
    "        labels    = batch[\"labels\"]\n",
    "\n",
    "        if max_input_ctx is not None:\n",
    "            input_ids = [ids[:max_input_ctx] for ids in input_ids]\n",
    "            attention = [att[:max_input_ctx] for att in attention]\n",
    "            # labels are not fed to generate, no need to truncate here\n",
    "\n",
    "        # 2) Pad this batch\n",
    "        max_len = max(len(x) for x in input_ids)\n",
    "        pad_id = tokenizer.pad_token_id\n",
    "        batch_input = [x + [pad_id]*(max_len - len(x)) for x in input_ids]\n",
    "        batch_attn  = [x + [0]*(max_len - len(x))  for x in attention]\n",
    "\n",
    "        batch_input = torch.tensor(batch_input, dtype=torch.long, device=device)\n",
    "        batch_attn  = torch.tensor(batch_attn,  dtype=torch.long, device=device)\n",
    "\n",
    "        # 3) Generate\n",
    "        gen = model.generate(\n",
    "            input_ids=batch_input,\n",
    "            attention_mask=batch_attn,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        # 4) Decode predictions (streaming: only this batch)\n",
    "        decoded_preds = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "        # 5) Prepare and decode labels for this batch (replace -100 with pad for decode)\n",
    "        #    (We never feed labels to the model here, just for metrics.)\n",
    "        max_lab = max(len(l) for l in labels)\n",
    "        labels_padded = [\n",
    "            [tok if tok != -100 else pad_id for tok in l] + [pad_id]*(max_lab - len(l))\n",
    "            for l in labels\n",
    "        ]\n",
    "        decoded_refs = tokenizer.batch_decode(\n",
    "            torch.tensor(labels_padded, dtype=torch.long), skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # 6) Minimal post-processing for ROUGE-Lsum\n",
    "        decoded_preds = [p.strip() for p in decoded_preds]\n",
    "        decoded_refs  = [r.strip() for r in decoded_refs]\n",
    "        decoded_preds = [\"\\n\".join(p.splitlines()) for p in decoded_preds]\n",
    "        decoded_refs  = [\"\\n\".join(r.splitlines()) for r in decoded_refs]\n",
    "\n",
    "        # 7) Stream into metrics (no big lists kept)\n",
    "        rouge_metric.add_batch(predictions=decoded_preds, references=decoded_refs)\n",
    "        bleu_metric.add_batch(predictions=decoded_preds, references=[[r] for r in decoded_refs])\n",
    "\n",
    "        # 8) Free everything we can\n",
    "        del batch_input, batch_attn, gen\n",
    "        if torch.cuda.is_available() and (start // batch_size) % clear_cuda_every == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # 9) Compute final metrics\n",
    "    rouge = rouge_metric.compute(use_stemmer=True)\n",
    "    bleu  = bleu_metric.compute()\n",
    "    return {\n",
    "        \"rouge1\": round(rouge[\"rouge1\"] * 100, 4),\n",
    "        \"rouge2\": round(rouge[\"rouge2\"] * 100, 4),\n",
    "        \"rougeL\": round(rouge[\"rougeL\"] * 100, 4),\n",
    "        \"rougeLsum\": round(rouge[\"rougeLsum\"] * 100, 4),\n",
    "        \"bleu\": round(bleu[\"bleu\"] * 100, 4),\n",
    "    }\n",
    "\n",
    "# ---- Usage example (super light) ----\n",
    "scores = streaming_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    val_ds,\n",
    "    n_samples=100,           # evaluate on a tiny slice\n",
    "    batch_size=1,            # minimize peak RAM\n",
    "    max_input_ctx=512,       # hard cap context if your eval_ds has longer\n",
    "    max_new_tokens=128,      # short generations\n",
    ")\n",
    "print(scores)\n",
    "# qwen 1.7-384ctx checkpoint 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd38c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(39.5134), 'rouge2': np.float64(39.2952), 'rougeL': np.float64(39.5022), 'rougeLsum': np.float64(39.3779), 'bleu': 22.8359}\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Reuse your tokenizer, model, and eval_ds (tokenized with input_ids, attention_mask, labels)\n",
    "# Assumptions:\n",
    "#   - eval_ds[i][\"input_ids\"] / [\"attention_mask\"] / [\"labels\"] are python lists of ints\n",
    "#   - tokenizer has pad_token_id and eos_token_id set\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleu_metric  = evaluate.load(\"bleu\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def streaming_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_ds,\n",
    "    *,\n",
    "    n_samples=256,              # limit for quick eval\n",
    "    batch_size=1,               # keep tiny\n",
    "    max_input_ctx=None,         # hard cap input ctx tokens (optional)\n",
    "    max_new_tokens=128,         # cap generated length\n",
    "    device=None,\n",
    "    clear_cuda_every=8,         # free CUDA cache periodically\n",
    "):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # Process in small slices, no big lists\n",
    "    end = min(n_samples, len(eval_ds))\n",
    "    for start in range(0, end, batch_size):\n",
    "        stop = min(start + batch_size, end)\n",
    "        batch = eval_ds.select(range(start, stop))\n",
    "\n",
    "        # 1) Truncate inputs (optional safety cap)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention = batch[\"attention_mask\"]\n",
    "        labels    = batch[\"labels\"]\n",
    "\n",
    "        if max_input_ctx is not None:\n",
    "            input_ids = [ids[:max_input_ctx] for ids in input_ids]\n",
    "            attention = [att[:max_input_ctx] for att in attention]\n",
    "            # labels are not fed to generate, no need to truncate here\n",
    "\n",
    "        # 2) Pad this batch\n",
    "        max_len = max(len(x) for x in input_ids)\n",
    "        pad_id = tokenizer.pad_token_id\n",
    "        batch_input = [x + [pad_id]*(max_len - len(x)) for x in input_ids]\n",
    "        batch_attn  = [x + [0]*(max_len - len(x))  for x in attention]\n",
    "\n",
    "        batch_input = torch.tensor(batch_input, dtype=torch.long, device=device)\n",
    "        batch_attn  = torch.tensor(batch_attn,  dtype=torch.long, device=device)\n",
    "\n",
    "        # 3) Generate\n",
    "        gen = model.generate(\n",
    "            input_ids=batch_input,\n",
    "            attention_mask=batch_attn,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        # 4) Decode predictions (streaming: only this batch)\n",
    "        decoded_preds = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "        # 5) Prepare and decode labels for this batch (replace -100 with pad for decode)\n",
    "        #    (We never feed labels to the model here, just for metrics.)\n",
    "        max_lab = max(len(l) for l in labels)\n",
    "        labels_padded = [\n",
    "            [tok if tok != -100 else pad_id for tok in l] + [pad_id]*(max_lab - len(l))\n",
    "            for l in labels\n",
    "        ]\n",
    "        decoded_refs = tokenizer.batch_decode(\n",
    "            torch.tensor(labels_padded, dtype=torch.long), skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # 6) Minimal post-processing for ROUGE-Lsum\n",
    "        decoded_preds = [p.strip() for p in decoded_preds]\n",
    "        decoded_refs  = [r.strip() for r in decoded_refs]\n",
    "        decoded_preds = [\"\\n\".join(p.splitlines()) for p in decoded_preds]\n",
    "        decoded_refs  = [\"\\n\".join(r.splitlines()) for r in decoded_refs]\n",
    "\n",
    "        # 7) Stream into metrics (no big lists kept)\n",
    "        rouge_metric.add_batch(predictions=decoded_preds, references=decoded_refs)\n",
    "        bleu_metric.add_batch(predictions=decoded_preds, references=[[r] for r in decoded_refs])\n",
    "\n",
    "        # 8) Free everything we can\n",
    "        del batch_input, batch_attn, gen\n",
    "        if torch.cuda.is_available() and (start // batch_size) % clear_cuda_every == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # 9) Compute final metrics\n",
    "    rouge = rouge_metric.compute(use_stemmer=True)\n",
    "    bleu  = bleu_metric.compute()\n",
    "    return {\n",
    "        \"rouge1\": round(rouge[\"rouge1\"] * 100, 4),\n",
    "        \"rouge2\": round(rouge[\"rouge2\"] * 100, 4),\n",
    "        \"rougeL\": round(rouge[\"rougeL\"] * 100, 4),\n",
    "        \"rougeLsum\": round(rouge[\"rougeLsum\"] * 100, 4),\n",
    "        \"bleu\": round(bleu[\"bleu\"] * 100, 4),\n",
    "    }\n",
    "\n",
    "# ---- Usage example (super light) ----\n",
    "scores = streaming_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    val_ds,\n",
    "    n_samples=100,           # evaluate on a tiny slice\n",
    "    batch_size=1,            # minimize peak RAM\n",
    "    max_input_ctx=512,       # hard cap context if your eval_ds has longer\n",
    "    max_new_tokens=128,      # short generations\n",
    ")\n",
    "print(scores)\n",
    "\n",
    "# qwen 0.6 checkpoint 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be0b7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_args = TrainingArguments(\n",
    "#     output_dir=\"./tmp-eval\",\n",
    "#     per_device_eval_batch_size=1,          # keep tiny\n",
    "#     # eval_accumulation_steps=1,             # stream metrics to CPU to save GPU RAM\n",
    "#     # dataloader_pin_memory=False,           # sometimes helps stability\n",
    "#     fp16=False,                            # keep off unless you know bf16/amp works\n",
    "#     bf16=False,\n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "\n",
    "# trainer=Trainer(\n",
    "#             model=model,\n",
    "#             args=eval_args,\n",
    "#             # train_dataset=train_ds,\n",
    "#             eval_dataset=val_ds,\n",
    "#             tokenizer=tokenizer,\n",
    "#             data_collator=data_collator,\n",
    "#             compute_metrics=compute_metrics)\n",
    "# n_samples = 4\n",
    "# subset = val_ds.select(range(min(n_samples, len(val_ds))))\n",
    "\n",
    "# scores = trainer.evaluate(\n",
    "#     subset,\n",
    "#     metric_key_prefix=\"quick_eval\",\n",
    "# )\n",
    "\n",
    "# print(\"Eval result:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8b93b",
   "metadata": {},
   "source": [
    "## slm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bbac7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khointn/anaconda3/envs/apple/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Tokenize train: 100%|██████████| 2615/2615 [00:15<00:00, 172.72 examples/s]\n",
      "Tokenize eval: 100%|██████████| 654/654 [00:03<00:00, 181.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    PreTrainedTokenizerBase,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"  # or \"Qwen/Qwen2.5-1.5B\"\n",
    "OUTPUT_DIR = \"qwen3-0.6b\"\n",
    "\n",
    "TEXT_COLUMN = os.environ.get(\"TEXT_COLUMN\", \"text\")\n",
    "SUMMARY_COLUMN = os.environ.get(\"SUMMARY_COLUMN\", \"summary\")\n",
    "\n",
    "MAX_INPUT_TOKENS = int(os.environ.get(\"MAX_INPUT_TOKENS\", 512))\n",
    "MAX_TARGET_TOKENS = int(os.environ.get(\"MAX_TARGET_TOKENS\", 128))\n",
    "MAX_SEQ_LEN = MAX_INPUT_TOKENS + MAX_TARGET_TOKENS\n",
    "\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", 2))\n",
    "GRAD_ACCUM = int(os.environ.get(\"GRAD_ACCUM\", 8))\n",
    "LR = float(os.environ.get(\"LR\", 2e-5))\n",
    "NUM_EPOCHS = float(os.environ.get(\"NUM_EPOCHS\", 1))\n",
    "WARMUP_RATIO = float(os.environ.get(\"WARMUP_RATIO\", 0.03))\n",
    "EVAL_STEPS = int(os.environ.get(\"EVAL_STEPS\", 20))\n",
    "LOGGING_STEPS = int(os.environ.get(\"LOGGING_STEPS\", 20))\n",
    "\n",
    "# Prompt format\n",
    "INSTR_PREFIX = (\n",
    "    \"You are a helpful assistant that writes concise, faithful summaries of legislative text.\\n\\n\"\n",
    "    \"Task: Read the following bill text and write a clear, accurate summary.\\n\\n\"\n",
    "    \"Bill Text:\\n\"\n",
    ")\n",
    "RESPONSE_PREFIX = \"\\n\\nSummary:\"  # we will mask loss before this\n",
    "\n",
    "# =======================\n",
    "# Tokenizer & Model\n",
    "# =======================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "# Many decoder-only tokenizers don't come with a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# Dataset\n",
    "# =======================\n",
    "raw = load_dataset(\"billsum\", split=\"test\")\n",
    "# small validation slice\n",
    "# if len(raw[\"train\"]) > 40000:\n",
    "#     val_size = 2000\n",
    "# else:\n",
    "#     val_size = max(1000, int(0.05 * len(raw[\"train\"])))\n",
    "split = raw.train_test_split(test_size=0.2, seed=42)\n",
    "train_ds = split[\"train\"]\n",
    "val_ds = split[\"test\"]\n",
    "\n",
    "# =======================\n",
    "# Build samples with masked labels\n",
    "# =======================\n",
    "response_template = RESPONSE_PREFIX + \" \"\n",
    "response_template_ids = tokenizer(response_template, add_special_tokens=False).input_ids\n",
    "\n",
    "def format_example(ex: Dict[str, str]) -> Tuple[str, str]:\n",
    "    src = (ex.get(TEXT_COLUMN) or \"\").strip()\n",
    "    tgt = (ex.get(SUMMARY_COLUMN) or \"\").strip()\n",
    "    prompt = f\"{INSTR_PREFIX}{src}{RESPONSE_PREFIX} \"\n",
    "    target = tgt\n",
    "    return prompt, target\n",
    "\n",
    "def tokenize_and_mask(example: Dict[str, str]) -> Dict[str, List[int]]:\n",
    "    prompt, target = format_example(example)\n",
    "\n",
    "    # Truncate prompt/target separately for better control\n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"][:MAX_INPUT_TOKENS]\n",
    "    target_ids = tokenizer(target, add_special_tokens=False)[\"input_ids\"][:MAX_TARGET_TOKENS]\n",
    "\n",
    "    input_ids = prompt_ids + target_ids + [tokenizer.eos_token_id]\n",
    "    input_ids = input_ids[:MAX_SEQ_LEN]\n",
    "\n",
    "    # Build labels: -100 over the prompt tokens, actual ids over the target\n",
    "    labels = [-100] * len(prompt_ids) + target_ids + [tokenizer.eos_token_id]\n",
    "    labels = labels[:MAX_SEQ_LEN]\n",
    "\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "train_ds = train_ds.map(tokenize_and_mask, remove_columns=train_ds.column_names, desc=\"Tokenize train\")\n",
    "val_ds = val_ds.map(tokenize_and_mask, remove_columns=val_ds.column_names, desc=\"Tokenize eval\")\n",
    "\n",
    "# =======================\n",
    "# Data collator (pads labels with -100)\n",
    "# =======================\n",
    "@dataclass\n",
    "class DataCollatorForCausalLMWithMaskedLabels:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    label_pad_token_id: int = -100\n",
    "    pad_to_multiple_of: int = 8\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # features: list of dicts with input_ids, labels, attention_mask\n",
    "        batch_input_ids, batch_labels, batch_attention = [], [], []\n",
    "        max_len = max(len(f[\"input_ids\"]) for f in features)\n",
    "        if self.pad_to_multiple_of:\n",
    "            # round up to nearest multiple\n",
    "            if max_len % self.pad_to_multiple_of != 0:\n",
    "                max_len = ((max_len // self.pad_to_multiple_of) + 1) * self.pad_to_multiple_of\n",
    "\n",
    "        for f in features:\n",
    "            ids = f[\"input_ids\"]\n",
    "            attn = f[\"attention_mask\"]\n",
    "            lbl = f[\"labels\"]\n",
    "\n",
    "            pad_len = max_len - len(ids)\n",
    "            batch_input_ids.append(ids + [self.tokenizer.pad_token_id] * pad_len)\n",
    "            batch_attention.append(attn + [0] * pad_len)\n",
    "            batch_labels.append(lbl + [self.label_pad_token_id] * pad_len)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(batch_input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(batch_attention, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(batch_labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "data_collator = DataCollatorForCausalLMWithMaskedLabels(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "# =======================\n",
    "# Metrics (ROUGE + BLEU)\n",
    "# =======================\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "def _postprocess_text(preds: List[str], labels: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    # ROUGE-Lsum expects sentence-per-line\n",
    "    preds = [\"\\n\".join(p.splitlines()) for p in preds]\n",
    "    labels = [\"\\n\".join(l.splitlines()) for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Replace -100 in labels so we can decode\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = _postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    rouge = rouge_metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    bleu = bleu_metric.compute(\n",
    "        predictions=decoded_preds, references=[[r] for r in decoded_labels]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": round(rouge[\"rouge1\"] * 100, 4),\n",
    "        \"rouge2\": round(rouge[\"rouge2\"] * 100, 4),\n",
    "        \"rougeL\": round(rouge[\"rougeL\"] * 100, 4),\n",
    "        \"rougeLsum\": round(rouge[\"rougeLsum\"] * 100, 4),\n",
    "        \"bleu\": round(bleu[\"bleu\"] * 100, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Training args\n",
    "# =======================\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=EVAL_STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "\n",
    "    # Enable generation-based eval\n",
    "    # predict_with_generate=True,\n",
    "    # generation_max_length=MAX_TARGET_TOKENS,\n",
    "    # generation_num_beams=1,\n",
    "\n",
    "    fp16=False,  # set bf16=True if your hardware supports it (and torch.bfloat16 above)\n",
    "    report_to=\"none\",\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# Trainer\n",
    "# =======================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save final\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"\\nDone. Model & tokenizer saved to:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb73867",
   "metadata": {},
   "source": [
    "## flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe30c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khointn/anaconda3/envs/apple/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841b307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 9192\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 2298\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billsum = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "billsum = billsum.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60b213a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a50eed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9192/9192 [00:09<00:00, 980.78 examples/s] \n",
      "Map: 100%|██████████| 2298/2298 [00:01<00:00, 1394.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"highlights\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b031a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7463f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 5.94kB [00:00, 8.83MB/s]\n",
      "Downloading extra modules: 3.34kB [00:00, 7.24MB/s]\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result_rouge = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result_rouge[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    result_bleu = bleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[ref] for ref in decoded_labels],\n",
    "    )\n",
    "\n",
    "    # Combine\n",
    "    return {\n",
    "        \"rouge1\": round(result_rouge[\"rouge1\"] * 100, 4),\n",
    "        \"rouge2\": round(result_rouge[\"rouge2\"] * 100, 4),\n",
    "        \"rougeL\": round(result_rouge[\"rougeL\"] * 100, 4),\n",
    "        \"rougeLsum\": round(result_rouge[\"rougeLsum\"] * 100, 4),\n",
    "        \"bleu\": round(result_bleu[\"bleu\"] * 100, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9079311",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc51b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"flan-t5\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, #change to bf16=True for XPU\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79617729",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f1164c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Your max_length is set to 200, but your input_length is only 103. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"the Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country.\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "trained_checkpoint = \"/home/khointn/summarization_adapter/nbs/my_awesome_billsum_model/checkpoint-620\"\n",
    "summarizer = pipeline(\"summarization\", model=trained_checkpoint)\n",
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa84946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apple",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
