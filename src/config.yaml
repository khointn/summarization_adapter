# Base model and output
PRETRAINED_MODEL: "Qwen/Qwen3-0.6B"
DATA_DIR: "./data"
OUTPUT_BASE_DIR: "./output_models"

# Data / sequence
MAX_INPUT_TOKENS: 384
MAX_TARGET_TOKENS: 128
TEXT_COL: "text"
SUMMARY_COL: "summary"

# Training
BATCH_SIZE: 2
GRAD_ACCUM: 8
LEARNING_RATE: 2e-5
EPOCHS: 4
WARMUP_RATIO: 0.03
EVAL_STEPS: 400
LOGGING_STEPS: 400
SEED: 42
LOAD_4BIT: true

# LoRA config
LORA_R: 8
LORA_ALPHA: 16
LORA_DROPOUT: 0.05

# Optimizer
OPTIM: "adamw_bnb_8bit"