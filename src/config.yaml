# wandb for logging
wandb_project: "summarization_adapter"

# base model and output
model: "Qwen/Qwen3-0.6B"
data_dir: "./data"
output_base_dir: "./output_models"

# data / sequence
max_input_tokens: 384
max_target_tokens: 128
text_col: "text"
summary_col: "summary"

# training
batch_size: 2
grad_accum: 8
learning_rate: 0.00002 # 2e-5
epochs: 4
warmup_ratio: 0.03
eval_steps: 400
logging_steps: 400
seed: 42
load_4bit: true

# lora config
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05

# optimizer
optim: "adamw_bnb_8bit"